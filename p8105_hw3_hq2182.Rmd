---
title: "p8105_hw3_hq2182"
author: "Hanfei Qi"
date: "10/6/2020"
output: github_document
---

Load libraries and do some figure setting.
```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
```

# Problem 1
Input the data set
```{r, message = FALSE, warning = FALSE}
data("instacart")
```

Description:

This data set contains `r nrow(instacart)` rows and `r ncol(instacart)` columns.
Observations are the level of items in orders by user. There are user / order variables -- user ID, order ID, order day, and order hour. There are also item variables -- name, aisle, department, and some numeric codes such as add to cart order, reordered, order number, and aisle id.


* How many aisles, and which are more items from?

```{r}
instacart %>%
  count(aisle) %>%
  arrange(desc(n))
```

Comment: There are 134 aisles, the top three aisles with more items are "fresh vegetables", "fresh fruits", and "packaged vegetables fruits". It looks like there are many types of vegetables and fruits.
  

* Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered. Arrange aisles sensibly, and organize your plot so others can read it.

```{r}
instacart %>%
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(
    aisle = factor(aisle),
    aisle = fct_reorder(aisle, n)
  ) %>% 
  ggplot(aes(x = aisle, y = n)) +
  geom_point() +
  labs(
    title = "Number of Items in Each Aisle",
    x = "Name of Aisle",
    y = "Number of Items") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

Comment: As I expected, vegetables, fruits, and yogurt have the most items, and the remaining aisles (#items > 10 thousands) have similar numbers of items.
  

* Make a tableMake a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.

```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(aisle, rank) %>% 
  knitr::kable()
```

Comment: It looks like baking requires lots of sugar. Dogs prefer chicken & rice. People tend to purchase organic fruits.
  

* Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week

```{r, warning = FALSE, message = FALSE}
instacart %>% 
	filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
	group_by(product_name, order_dow) %>% 
	summarize(mean_hour = mean(order_hour_of_day)) %>% 
  mutate(
    order_dow = recode(
      order_dow,
      `0` = "Sun", `1` = "Mon", `2` = "Tues", `3` = "Wed", `4` = "Thur", `5` = "Fri", `6` = "Sat"
    )
  ) %>% 
	pivot_wider(
		names_from = order_dow,
		values_from = mean_hour
	)
```

Comment: Most of time the coffee ice cream is sold in the afternoon, maybe it's because the temperature is warmer in the afternoon. Pink lady apples is sold in the noon, maybe people buy it for lunch.  

# Problem 2

* Load, tidy, and otherwise wrangle the data.
```{r, message = FALSE}
accel_df = 
  read_csv(
    file = "./accel_data.csv",
    col_names = TRUE) %>%
  janitor::clean_names() %>%
  pivot_longer(
    activity_1:activity_1440,
    names_to = "activity",
    values_to = "activity_number"
  ) %>% 
  mutate(
    day_id = as.factor(day_id),
    week = as.factor(week),
    weekday = case_when(
      day == "Monday" ~ "yes",
      day == "Tuesday" ~ "yes",
      day == "Wednesday" ~ "yes",
      day == "Thurday" ~ "yes",
      day == "Friday" ~ "yes",
      TRUE            ~ "no"),
    weekend = case_when(
      day == "Saturday" ~ "yes",
      day == "Sunday" ~ "yes",
      TRUE           ~ "no"
    ))
```

Description: This data set contains `r nrow(accel_df)` rows and `r ncol(accel_df)` columns.  

* The following are variables in this data set:
  * week: The number of week since the experiment started
  * day_id: Unique id of certain day, the number is number of day after starting experiment
  * day: Name of weekday
  * activity: Count of minutes starting at midnight
  * activity_number: Counts of each activity of certain time
  * weekday: "yes" if the date is weekday, "no" if the date is weekend
  * weekend: "yes" if the date is weekend, "no" if the date is weekday 


* Add variable of total activity for each day, then make a table

```{r, message = FALSE}
accel_df %>% 
  group_by(day_id, day) %>% 
  summarize(
    sum_act = sum(activity_number)
  ) %>% 
  arrange(
    desc(sum_act)
  ) %>% 
  knitr::kable()
```

Comment: It looks like this person was more active on Mon, Wed, and Fri, so he/she preferred doing exercise every other day. It seems that he/she tried to keep regular exercise. 


* Make a plot shows 24-hour activity time courses for each day
* Colors indicate day of the week

```{r}
accel_df %>% 
  group_by(day_id) %>% 
  ggplot(aes(x = activity, y = activity_number)) +
    geom_point(aes(color = day), alpha = .5) +
  labs(
    title = "24-hour activity time courses for each day",
    x = "Time Line Starting at Midnight",
    y = "Activity Numbers") +
    theme(axis.text.x = element_blank())
```

Comment: This person usually did activity after waking up and before lunch. He/She was also active before going to bed. As I expected, he/she did most exercise on Mon, Wed and Fri.


# Problem 3

Input and observe the data
```{r}
library(patchwork)
library(ggridges)
data("ny_noaa")
summary(ny_noaa)
```

Description: This data contains 7 variables: weather station id, date of observation, precipitation (tenth of mm), snowfall (mm), snow depth (mm), max/min temperature (tenths of degree C). There are `r nrow(ny_noaa)`` rows in the data. The amount of NA values in max/min temperature may make troubles when we want to analyze temp vs. precipitation / snowfall.


* Separate variables for year, month, and day
* Ensure units of temperature, precipitation, and snowfall
* Observe common values for snowfall

```{r}
sep_date = ny_noaa %>% 
  separate(col = date, 
           into = c("year", "month", "day")) %>% 
  mutate(
   prcp = prcp / 10,
   tmax_num = as.numeric(tmax) / 10,
   tmin_num = as.numeric(tmin) / 10
  ) 

summary(sep_date$snow) 

getmode = function(v) {
   uniqv = unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}
getmode(sep_date$snow)
```

Comment: There are lots of NA's of snow, 381221 out of 2595176 data points. Without NAs, the most common value of snow fall is 0. It's probably because there's no snowfall at most day so they just don't record it. The minimum value is -13, which is impossible, so the data set contains some errors.


* Make a two-panel plot showing the average max temperature in January and in July in each station across years.

```{r}
jan_df = sep_date %>% 
  group_by(id, month, year) %>% 
  summarize(mean_tmax = mean(tmax_num)) %>% 
  filter(month == "01") %>% 
  drop_na()

jul_df = sep_date %>% 
  group_by(id, month, year) %>% 
  summarize(mean_tmax = mean(tmax_num)) %>% 
  filter(month == "07") %>% 
  drop_na()

jan_p = 
  jan_df %>% 
  ggplot(aes(x = year, y = mean_tmax, group = id)) +
  geom_point(alpha = .5) +
  geom_path() +
  labs(
    title = "Avg. Temperature of January in Each Station Across Years",
    y = "avg. temperature of Jan of each station") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))

jul_p = 
  jul_df %>% 
  ggplot(aes(x = year, y = mean_tmax, group = id)) +
  geom_point(alpha = .5) +
  geom_path() +
  labs(
    title = "Avg. Temperature of July in Each Station Across Years",
    y = "avg. temperature of Jul of each station") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))

jan_p / jul_p
```

Comment: The average temparature in Jan was around 0 ~ - 5 degree C. The temperature in Jan 1982, 1994, and 2004 were extremely low. It should be noticed that one station in Jan 1982 had -16 degree C ave. temperature. The normal Temperature in Jul was around 25 ~ 28 degree C. There was stations had extremely low temperature in Jul 1984, 1988, 2004, and 2007. Overall, the average temperature across year were like sin-wave, meaning that average temperature went up and down regularly.


* Make a two-panel plot: tmax vs tmin for the full data set.
* Make a plot: the distribution of snowfall values greater than 0 and less than 100 separately by year. 

```{r}
tmax_tmin_p = 
  sep_date %>% 
  drop_na() %>% 
  ggplot(aes(x = tmin_num, y = tmax_num)) +
  geom_hex() +
  labs(
    title = "tmax vs. tmin for the full dataset",
    x = "Minimum Temperature",
    y = "Maximum Temperature")
  
```

```{r, warning = FALSE}
distr_snow_p = 
  sep_date %>% 
  filter(snow > 0 &
           snow < 100) %>% 
  drop_na(snow) %>% 
  ggplot(
    aes(x = year, y = snow)
    ) +
  geom_violin(aes(fill = year), alpha = .5) +
  stat_summary(fun = "mean", color = "black") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  labs(
    title = "Distribution of Snowfall >0 & <100 Across Years",
    y = "snowfall (mm)") +
  theme(legend.position = "bottom",
        legend.text = element_text(size = 7),
        legend.key.size = unit(0.5, "lines"))

tmax_tmin_p / distr_snow_p
```

Comment: As the minimum temperature increases, the maximum temperature also increases. There are some extreme numbers, probably due to extreme weather or error in the data. The amount of snowfall are constant across years.

