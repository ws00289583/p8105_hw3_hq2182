---
title: "p8105_hw3_hq2182"
author: "Hanfei Qi"
date: "10/6/2020"
output: github_document
---

Load libraries and do some figure setting.
```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
```

# Problem 1
Input the data set
```{r, message = FALSE, warning = FALSE}
data("instacart")
```

Description:

This data set contains `r nrow(instacart)` rows and `r ncol(instacart)` columns.
Observations are the level of items in orders by user. There are user / order variables -- user ID, order ID, order day, and order hour. There are also item variables -- name, aisle, department, and some numeric codes such as add to cart order, reordered, order number, and aisle id.


* How many aisles, and which are more items from?

```{r}
instacart %>%
  count(aisle) %>%
  arrange(desc(n))
```

Comment: There are 134 aisles, the top three aisles with more items are "fresh vegetables", "fresh fruits", and "packaged vegetables fruits". It looks like there are many types of vegetables and fruits.
  

* Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered. Arrange aisles sensibly, and organize your plot so others can read it.

```{r}
instacart %>%
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(
    aisle = factor(aisle),
    aisle = fct_reorder(aisle, n)
  ) %>% 
  ggplot(aes(x = aisle, y = n)) +
  geom_point() +
  labs(
    title = "Number of Items in Each Aisle",
    x = "Name of Aisle",
    y = "Number of Items") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

Comment: As I expected, vegetables, fruits, and yogurt have the most items, and the remaining aisles (#items > 10 thousands) have similar numbers of items.
  

* Make a tableMake a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”.

```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(aisle, rank) %>% 
  knitr::kable()
```

Comment: It looks like baking requires lots of sugar. Dogs prefer chicken & rice. People tend to purchase organic fruits.
  

* Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week

```{r, warning = FALSE, message = FALSE}
instacart %>% 
	filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
	group_by(product_name, order_dow) %>% 
	summarize(mean_hour = mean(order_hour_of_day)) %>% 
  mutate(
    order_dow = recode(
      order_dow,
      `0` = "Sun", `1` = "Mon", `2` = "Tues", `3` = "Wed", `4` = "Thur", `5` = "Fri", `6` = "Sat"
    )
  ) %>% 
	pivot_wider(
		names_from = order_dow,
		values_from = mean_hour
	)
```

Comment: Most of time the coffee ice cream is sold in the afternoon, maybe it's because the temperature is warmer in the afternoon. Pink lady apples is sold in the noon, maybe people buy it for lunch.  

# Problem 2

* Load, tidy, and otherwise wrangle the data.
```{r, message = FALSE}
accel_df = 
  read_csv(
    file = "./accel_data.csv",
    col_names = TRUE) %>%
  janitor::clean_names() %>%
  pivot_longer(
    activity_1:activity_1440,
    names_to = "activity",
    values_to = "activity_number"
  ) %>% 
  mutate(
    day_id = as.factor(day_id),
    week = as.factor(week),
    weekday = case_when(
      day == "Monday" ~ "yes",
      day == "Tuesday" ~ "yes",
      day == "Wednesday" ~ "yes",
      day == "Thurday" ~ "yes",
      day == "Friday" ~ "yes",
      TRUE            ~ "no"),
    weekend = case_when(
      day == "Saturday" ~ "yes",
      day == "Sunday" ~ "yes",
      TRUE           ~ "no"
    ))
```

Description: This data set contains `r nrow(accel_df)` rows and `r ncol(accel_df)` columns.  

* The following are variables in this data set:
  * week: The number of week since the experiment started
  * day_id: Unique id of certain day, the number is number of day after starting experiment
  * day: Name of weekday
  * activity_*: Names of activities, but do not know what is the specific activity
  * activity_number: Counts of each activity
  * weekday: "yes" if the date is weekday, "no" if the date is weekend
  * weekend: "yes" if the date is weekend, "no" if the date is weekday 


* Add variable of total activity for each day, then make a table

```{r, message = FALSE}
accel_df %>% 
  group_by(day_id, day) %>% 
  summarize(
    sum_act = sum(activity_number)
  ) %>% 
  arrange(
    desc(sum_act)
  ) %>% 
  knitr::kable()
```

Comment:


* Make a plot shows 24-hour activity time courses for each day
* Colors indicate day of the week

```{r}
accel_df %>% 
  group_by(day_id) %>% 
  ggplot(aes(x = activity, y = activity_number)) +
    geom_point(aes(color = day), alpha = .5) +
  labs(
    title = "24-hour activity time courses for each day",
    x = "Time Line Starting at Midnight",
    y = "Activity Numbers") +
    theme(axis.text.x = element_blank())
```

Comment:


# Problem 3

Input and observe the data
```{r}
library(patchwork)
library(ggridges)
data("ny_noaa")
summary(ny_noaa)
```

Description: This data contains 7 variables: weather station id, date of observation, precipitation (tenth of mm), snowfall (mm), snow depth (mm), max/min temperature (tenths of degree C). There are `r nrow(ny_noaa)`` rows in the data. The amount of NA values in max/min temperature may make troubles when we want to analyze temp vs. precipitation / snowfall.


* Separate variables for year, month, and day
* Ensure units of temperature, precipitation, and snowfall
* Observe common values for snowfall

```{r}
sep_date = ny_noaa %>% 
  separate(col = date, 
           into = c("year", "month", "day")) %>% 
  mutate(
   prcp = prcp / 10,
   tmax_num = as.numeric(tmax) / 10,
   tmin_num = as.numeric(tmin) / 10
  ) 

summary(sep_date$snow) 

getmode = function(v) {
   uniqv = unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}
getmode(sep_date$snow)
```

Comment:


* Make a two-panel plot showing the average max temperature in January and in July in each station across years.

```{r}
jun_df = sep_date %>% 
  group_by(id, month, year) %>% 
  summarize(mean_tmax = mean(tmax_num)) %>% 
  filter(month == "06") %>% 
  drop_na()

jul_df = sep_date %>% 
  group_by(id, month, year) %>% 
  summarize(mean_tmax = mean(tmax_num)) %>% 
  filter(month == "07") %>% 
  drop_na()

jun_p = 
  jun_df %>% 
  ggplot(aes(x = year, y = mean_tmax, group = id)) +
  geom_point(alpha = .5) +
  geom_path() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))

jul_p = 
  jul_df %>% 
  ggplot(aes(x = year, y = mean_tmax, group = id)) +
  geom_point(alpha = .5) +
  geom_path() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))

jun_p / jul_p
```

Comment: 


* Make a two-panel plot: tmax vs tmin for the full dataset.
* Make a plot: the distribution of snowfall values greater than 0 and less than 100 separately by year.

```{r}
tmax_tmin_p = 
  sep_date %>% 
  drop_na() %>% 
  ggplot(aes(x = tmin_num, y = tmax_num)) +
  geom_hex() +
  labs(
    title = "tmax vs. tmin for the full dataset",
    x = "Minimum Temperature",
    y = "Maximum Temperature")
  
```

```{r, warning = FALSE}
distr_snow_p = 
  sep_date %>% 
  filter(snow > 0 &
           snow < 100) %>% 
  drop_na(snow) %>% 
  ggplot(
    aes(x = year, y = snow)
    ) +
  geom_violin(aes(fill = year), alpha = .5) +
  stat_summary(fun = "median", color = "black") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  theme(legend.position = "bottom",
        legend.text = element_text(size = 7),
        legend.key.size = unit(0.5, "lines"))

tmax_tmin_p / distr_snow_p
```

Comment:

